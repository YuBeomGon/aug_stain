{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e4aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from dataset import val_transforms, CDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f1ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict \n",
    "args = easydict.EasyDict({ \"batch_size\": 256, \n",
    "                          \"epochs\": 30, \n",
    "                          \"data\": 0, \n",
    "                          'lr':0.1,\n",
    "                         'momentum':0.9,\n",
    "                         'weight_decay':1e-4,\n",
    "                         'start_epoch':0,\n",
    "                         'gpu':0,\n",
    "                          'workers':16,\n",
    "                         'print_freq':2000,\n",
    "                         'saved_dir':'../trained_models/checkpoint.pt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f5ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make saved dir\n",
    "from pathlib import Path\n",
    "path = Path(args.saved_dir.split('checkpoint')[0])\n",
    "path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d1b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "ngpus_per_node = torch.cuda.device_count()\n",
    "print(ngpus_per_node)\n",
    "GPU_NUM = args.gpu # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c55e114-1622-42c4-9ce1-e1835910c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from augment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e52944",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE, p=1),\n",
    "    A.OneOf([\n",
    "        A.HorizontalFlip(p=.8),\n",
    "        A.VerticalFlip(p=.8),\n",
    "        A.RandomRotate90(p=.8)]\n",
    "    ),\n",
    "    # HEColorAugment(sigma1=.4, sigma2=5., mat=None, p=0.9),\n",
    "], p=1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "732a5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import cv2\n",
    "\n",
    "class CDataset(Dataset):\n",
    "    def __init__(self, df, defaultpath='/home/beomgon/Dataset/new_patches/', transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.dir = defaultpath\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)   \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx, 4]\n",
    "#         print(pid)\n",
    "\n",
    "        image = cv2.imread(self.dir + path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         image = (image.astype(np.float32)-128.)/128.\n",
    "        \n",
    "#         if image is uint8, normalization by 255 is done automatically by albumebtation(ToTensor method)\n",
    "        if self.transform:\n",
    "            timage = self.transform(image=image)\n",
    "            image = timage['image']\n",
    "        \n",
    "        image =  torch.tensor(image, dtype=torch.float32)/255.\n",
    "        #image = (torch.tensor(image, dtype=torch.float32)-128)/128\n",
    "        image = image.permute(2,0,1)\n",
    "            \n",
    "        label = self.df.iloc[idx, 5]\n",
    "        return image, label, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada50d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../dataframe/train_New_Colon_df.csv')\n",
    "train_dataset = CDataset(train_df, defaultpath='/home/beomgon/Dataset/seegene/new_patches/', \n",
    "                         transform=train_transforms)\n",
    "\n",
    "val_df = pd.read_csv('../dataframe/val_New_Colon_df.csv')\n",
    "val_dataset = CDataset(val_df, defaultpath='/home/beomgon/Dataset/seegene/new_patches/',\n",
    "                       transform=val_transforms)  \n",
    "\n",
    "test_df = pd.read_csv('../dataframe/test_New_Colon_df.csv')\n",
    "test_dataset = CDataset(test_df, defaultpath='/home/beomgon/Dataset/seegene/new_patches/',\n",
    "                        transform=val_transforms)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfca7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label, path = next(iter(train_dataset))\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6787059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                          shuffle=True, num_workers=args.workers)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size,\n",
    "                          shuffle=True, num_workers=args.workers)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                          shuffle=True, num_workers=args.workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dffcb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels, paths = next(iter(train_loader))\n",
    "# images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfccf8f5-24fb-4986-9ad0-91ce95498912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import BasicBlock, conv1x1, conv3x3, Bottleneck\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.hub import load_state_dict_from_url\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "#         _log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.LT = nn.Parameter(torch.eye(3, dtype=torch.float), requires_grad=True)\n",
    "        self.LT_ma = torch.eye(3, dtype=torch.float, device='cuda')\n",
    "#         self.bn0 = norm_layer(3)\n",
    "#         self.scale = nn.Parameter(torch.tensor([1., 1., 1,]), requires_grad=True)\n",
    "#         self.bias = nn.Parameter(torch.tensor([0., 0., 0,]), requires_grad=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 64, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet(\"resnet18\", BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3f0d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.models as models\n",
    "from resnet import resnet18\n",
    "model = resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(64, 3)\n",
    "torch.nn.init.normal_(model.fc.weight, std=0.01)\n",
    "torch.nn.init.constant_(model.fc.bias, 0.)\n",
    "default_state_dict = model.state_dict()\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.Adam(params, lr=0.01, weight_decay=args.weight_decay)\n",
    "optimizer = torch.optim.SGD(\n",
    "       params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "# optimizer = torch.optim.SGD(params, lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15, 20, 25, 30], \n",
    "                                                    gamma=0.2)\n",
    "# criterion = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e0cf189-d9d0-4d14-bcb8-2071d3497b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21047c40-140f-4212-b841-7e9ac1bd7503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][  0/375]\tTime  3.268 ( 3.268)\tData  2.151 ( 2.151)\tLoss 1.1112e+00 (1.1112e+00)\tAcc@1  26.95 ( 26.95)\n",
      "Test: [  0/152]\tTime  2.204 ( 2.204)\tLoss 4.6042e-01 (4.6042e-01)\tAcc@1  84.38 ( 84.38)\n",
      " * Acc@1 84.537\n",
      "Epoch: [1][  0/375]\tTime  2.759 ( 2.759)\tData  2.551 ( 2.551)\tLoss 1.3304e-01 (1.3304e-01)\tAcc@1  94.92 ( 94.92)\n",
      "Test: [  0/152]\tTime  2.512 ( 2.512)\tLoss 3.4119e-01 (3.4119e-01)\tAcc@1  89.84 ( 89.84)\n",
      " * Acc@1 86.447\n",
      "Epoch: [2][  0/375]\tTime  2.910 ( 2.910)\tData  2.693 ( 2.693)\tLoss 1.0050e-01 (1.0050e-01)\tAcc@1  95.70 ( 95.70)\n",
      "Test: [  0/152]\tTime  2.456 ( 2.456)\tLoss 1.1550e+00 (1.1550e+00)\tAcc@1  73.44 ( 73.44)\n",
      " * Acc@1 75.198\n",
      "Epoch: [3][  0/375]\tTime  2.005 ( 2.005)\tData  1.795 ( 1.795)\tLoss 7.8396e-02 (7.8396e-02)\tAcc@1  97.27 ( 97.27)\n",
      "Test: [  0/152]\tTime  2.053 ( 2.053)\tLoss 1.2751e+00 (1.2751e+00)\tAcc@1  80.08 ( 80.08)\n",
      " * Acc@1 81.512\n",
      "Epoch: [4][  0/375]\tTime  2.045 ( 2.045)\tData  1.822 ( 1.822)\tLoss 9.3639e-02 (9.3639e-02)\tAcc@1  96.88 ( 96.88)\n",
      "Test: [  0/152]\tTime  2.492 ( 2.492)\tLoss 8.9297e+00 (8.9297e+00)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 52.318\n",
      "Epoch: [5][  0/375]\tTime  2.299 ( 2.299)\tData  2.089 ( 2.089)\tLoss 1.3521e-01 (1.3521e-01)\tAcc@1  94.92 ( 94.92)\n",
      "Test: [  0/152]\tTime  3.158 ( 3.158)\tLoss 1.8111e+00 (1.8111e+00)\tAcc@1  56.25 ( 56.25)\n",
      " * Acc@1 57.902\n",
      "Epoch: [6][  0/375]\tTime  2.337 ( 2.337)\tData  2.126 ( 2.126)\tLoss 7.0082e-02 (7.0082e-02)\tAcc@1  96.48 ( 96.48)\n",
      "Test: [  0/152]\tTime  2.398 ( 2.398)\tLoss 6.2024e+00 (6.2024e+00)\tAcc@1  50.00 ( 50.00)\n",
      " * Acc@1 50.937\n",
      "Epoch: [7][  0/375]\tTime  2.254 ( 2.254)\tData  2.055 ( 2.055)\tLoss 6.9970e-02 (6.9970e-02)\tAcc@1  97.66 ( 97.66)\n",
      "Test: [  0/152]\tTime  2.792 ( 2.792)\tLoss 9.2903e-01 (9.2903e-01)\tAcc@1  77.73 ( 77.73)\n",
      " * Acc@1 80.439\n",
      "Epoch: [8][  0/375]\tTime  2.184 ( 2.184)\tData  1.937 ( 1.937)\tLoss 4.2915e-02 (4.2915e-02)\tAcc@1  97.66 ( 97.66)\n",
      "Test: [  0/152]\tTime  1.690 ( 1.690)\tLoss 2.9708e+00 (2.9708e+00)\tAcc@1  58.98 ( 58.98)\n",
      " * Acc@1 63.163\n",
      "Epoch: [9][  0/375]\tTime  2.317 ( 2.317)\tData  2.106 ( 2.106)\tLoss 8.2222e-02 (8.2222e-02)\tAcc@1  97.27 ( 97.27)\n",
      "Test: [  0/152]\tTime  2.721 ( 2.721)\tLoss 7.4945e-01 (7.4945e-01)\tAcc@1  77.73 ( 77.73)\n",
      " * Acc@1 81.773\n",
      "Epoch: [10][  0/375]\tTime  2.754 ( 2.754)\tData  2.536 ( 2.536)\tLoss 1.0986e-01 (1.0986e-01)\tAcc@1  97.27 ( 97.27)\n",
      "Test: [  0/152]\tTime  2.649 ( 2.649)\tLoss 1.6591e-01 (1.6591e-01)\tAcc@1  94.53 ( 94.53)\n",
      " * Acc@1 95.951\n",
      "Epoch: [11][  0/375]\tTime  2.257 ( 2.257)\tData  2.045 ( 2.045)\tLoss 2.3717e-02 (2.3717e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.070 ( 2.070)\tLoss 3.2803e-01 (3.2803e-01)\tAcc@1  92.19 ( 92.19)\n",
      " * Acc@1 91.251\n",
      "Epoch: [12][  0/375]\tTime  2.118 ( 2.118)\tData  1.947 ( 1.947)\tLoss 4.4448e-02 (4.4448e-02)\tAcc@1  97.66 ( 97.66)\n",
      "Test: [  0/152]\tTime  2.283 ( 2.283)\tLoss 4.4875e-01 (4.4875e-01)\tAcc@1  88.67 ( 88.67)\n",
      " * Acc@1 87.401\n",
      "Epoch: [13][  0/375]\tTime  3.447 ( 3.447)\tData  3.214 ( 3.214)\tLoss 3.9861e-02 (3.9861e-02)\tAcc@1  98.44 ( 98.44)\n",
      "Test: [  0/152]\tTime  2.103 ( 2.103)\tLoss 4.4785e-01 (4.4785e-01)\tAcc@1  90.62 ( 90.62)\n",
      " * Acc@1 92.040\n",
      "Epoch: [14][  0/375]\tTime  2.074 ( 2.074)\tData  1.861 ( 1.861)\tLoss 1.2337e-02 (1.2337e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.201 ( 2.201)\tLoss 4.9911e-01 (4.9911e-01)\tAcc@1  90.62 ( 90.62)\n",
      " * Acc@1 90.256\n",
      "Epoch: [15][  0/375]\tTime  2.805 ( 2.805)\tData  2.608 ( 2.608)\tLoss 9.9450e-03 (9.9450e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.399 ( 2.399)\tLoss 1.2324e-01 (1.2324e-01)\tAcc@1  96.88 ( 96.88)\n",
      " * Acc@1 96.466\n",
      "Epoch: [16][  0/375]\tTime  2.701 ( 2.701)\tData  2.522 ( 2.522)\tLoss 2.0684e-02 (2.0684e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.500 ( 2.500)\tLoss 1.6906e-01 (1.6906e-01)\tAcc@1  95.70 ( 95.70)\n",
      " * Acc@1 94.558\n",
      "Epoch: [17][  0/375]\tTime  3.188 ( 3.188)\tData  2.982 ( 2.982)\tLoss 7.0465e-03 (7.0465e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.151 ( 2.151)\tLoss 7.0927e-02 (7.0927e-02)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 96.859\n",
      "Epoch: [18][  0/375]\tTime  2.943 ( 2.943)\tData  2.643 ( 2.643)\tLoss 5.2775e-03 (5.2775e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  1.855 ( 1.855)\tLoss 3.5423e-01 (3.5423e-01)\tAcc@1  93.36 ( 93.36)\n",
      " * Acc@1 95.023\n",
      "Epoch: [19][  0/375]\tTime  2.319 ( 2.319)\tData  2.138 ( 2.138)\tLoss 5.7833e-03 (5.7833e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  3.145 ( 3.145)\tLoss 3.0954e-01 (3.0954e-01)\tAcc@1  91.80 ( 91.80)\n",
      " * Acc@1 92.619\n",
      "Epoch: [20][  0/375]\tTime  2.592 ( 2.592)\tData  2.297 ( 2.297)\tLoss 2.3713e-02 (2.3713e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.618 ( 2.618)\tLoss 2.6996e-01 (2.6996e-01)\tAcc@1  94.14 ( 94.14)\n",
      " * Acc@1 94.938\n",
      "Epoch: [21][  0/375]\tTime  2.650 ( 2.650)\tData  2.455 ( 2.455)\tLoss 8.4876e-03 (8.4876e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.128 ( 2.128)\tLoss 1.2122e-01 (1.2122e-01)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 95.739\n",
      "Epoch: [22][  0/375]\tTime  2.686 ( 2.686)\tData  2.432 ( 2.432)\tLoss 9.0721e-03 (9.0721e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.508 ( 2.508)\tLoss 2.1573e-01 (2.1573e-01)\tAcc@1  94.14 ( 94.14)\n",
      " * Acc@1 96.590\n",
      "Epoch: [23][  0/375]\tTime  3.034 ( 3.034)\tData  2.785 ( 2.785)\tLoss 5.5671e-03 (5.5671e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.439 ( 2.439)\tLoss 6.6413e-02 (6.6413e-02)\tAcc@1  97.66 ( 97.66)\n",
      " * Acc@1 96.505\n",
      "Epoch: [24][  0/375]\tTime  1.988 ( 1.988)\tData  1.792 ( 1.792)\tLoss 2.0146e-02 (2.0146e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.250 ( 2.250)\tLoss 1.0218e-01 (1.0218e-01)\tAcc@1  96.88 ( 96.88)\n",
      " * Acc@1 96.598\n",
      "Epoch: [25][  0/375]\tTime  2.907 ( 2.907)\tData  2.691 ( 2.691)\tLoss 4.1438e-03 (4.1438e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.646 ( 2.646)\tLoss 1.6763e-01 (1.6763e-01)\tAcc@1  96.88 ( 96.88)\n",
      " * Acc@1 96.781\n",
      "Epoch: [26][  0/375]\tTime  2.749 ( 2.749)\tData  2.524 ( 2.524)\tLoss 1.4551e-02 (1.4551e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.397 ( 2.397)\tLoss 1.0894e-01 (1.0894e-01)\tAcc@1  95.70 ( 95.70)\n",
      " * Acc@1 96.691\n",
      "Epoch: [27][  0/375]\tTime  2.735 ( 2.735)\tData  2.529 ( 2.529)\tLoss 5.6988e-03 (5.6988e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.734 ( 2.734)\tLoss 1.2060e-01 (1.2060e-01)\tAcc@1  97.66 ( 97.66)\n",
      " * Acc@1 96.711\n",
      "Epoch: [28][  0/375]\tTime  2.743 ( 2.743)\tData  2.466 ( 2.466)\tLoss 3.5264e-02 (3.5264e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.635 ( 2.635)\tLoss 8.7716e-02 (8.7716e-02)\tAcc@1  97.66 ( 97.66)\n",
      " * Acc@1 96.807\n",
      "Epoch: [29][  0/375]\tTime  2.331 ( 2.331)\tData  2.030 ( 2.030)\tLoss 1.0593e-02 (1.0593e-02)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.136 ( 2.136)\tLoss 1.5114e-01 (1.5114e-01)\tAcc@1  96.09 ( 96.09)\n",
      " * Acc@1 96.655\n",
      "Test: [ 0/89]\tTime  2.303 ( 2.303)\tLoss 4.6517e-02 (4.6517e-02)\tAcc@1  98.05 ( 98.05)\n",
      " * Acc@1 97.393\n",
      "************* test number 0 ratio 0.8 acc97.3926773071289 *************\n",
      "Epoch: [0][  0/375]\tTime  2.799 ( 2.799)\tData  2.523 ( 2.523)\tLoss 1.1053e+00 (1.1053e+00)\tAcc@1  26.56 ( 26.56)\n",
      "Test: [  0/152]\tTime  2.504 ( 2.504)\tLoss 1.6495e+00 (1.6495e+00)\tAcc@1  47.66 ( 47.66)\n",
      " * Acc@1 51.041\n",
      "Epoch: [1][  0/375]\tTime  2.056 ( 2.056)\tData  1.840 ( 1.840)\tLoss 8.8214e-02 (8.8214e-02)\tAcc@1  95.70 ( 95.70)\n",
      "Test: [  0/152]\tTime  2.819 ( 2.819)\tLoss 2.7364e-01 (2.7364e-01)\tAcc@1  90.62 ( 90.62)\n",
      " * Acc@1 92.606\n",
      "Epoch: [2][  0/375]\tTime  2.746 ( 2.746)\tData  2.500 ( 2.500)\tLoss 1.1455e-01 (1.1455e-01)\tAcc@1  96.09 ( 96.09)\n",
      "Test: [  0/152]\tTime  2.518 ( 2.518)\tLoss 9.5631e-01 (9.5631e-01)\tAcc@1  64.84 ( 64.84)\n",
      " * Acc@1 67.765\n",
      "Epoch: [3][  0/375]\tTime  2.453 ( 2.453)\tData  2.108 ( 2.108)\tLoss 1.5614e-01 (1.5614e-01)\tAcc@1  94.14 ( 94.14)\n",
      "Test: [  0/152]\tTime  2.026 ( 2.026)\tLoss 2.0417e+00 (2.0417e+00)\tAcc@1  71.88 ( 71.88)\n",
      " * Acc@1 71.923\n",
      "Epoch: [4][  0/375]\tTime  2.822 ( 2.822)\tData  2.599 ( 2.599)\tLoss 6.2578e-02 (6.2578e-02)\tAcc@1  98.05 ( 98.05)\n",
      "Test: [  0/152]\tTime  3.151 ( 3.151)\tLoss 1.0011e+00 (1.0011e+00)\tAcc@1  76.17 ( 76.17)\n",
      " * Acc@1 72.256\n",
      "Epoch: [5][  0/375]\tTime  2.132 ( 2.132)\tData  1.936 ( 1.936)\tLoss 4.9634e-02 (4.9634e-02)\tAcc@1  98.44 ( 98.44)\n",
      "Test: [  0/152]\tTime  2.016 ( 2.016)\tLoss 2.8781e+00 (2.8781e+00)\tAcc@1  65.23 ( 65.23)\n",
      " * Acc@1 65.511\n",
      "Epoch: [6][  0/375]\tTime  2.526 ( 2.526)\tData  2.322 ( 2.322)\tLoss 1.5961e-01 (1.5961e-01)\tAcc@1  95.70 ( 95.70)\n",
      "Test: [  0/152]\tTime  2.035 ( 2.035)\tLoss 1.9871e+00 (1.9871e+00)\tAcc@1  73.44 ( 73.44)\n",
      " * Acc@1 70.147\n",
      "Epoch: [7][  0/375]\tTime  2.551 ( 2.551)\tData  2.260 ( 2.260)\tLoss 6.3191e-02 (6.3191e-02)\tAcc@1  98.05 ( 98.05)\n",
      "Test: [  0/152]\tTime  2.825 ( 2.825)\tLoss 1.3371e+00 (1.3371e+00)\tAcc@1  74.22 ( 74.22)\n",
      " * Acc@1 74.407\n",
      "Epoch: [8][  0/375]\tTime  1.867 ( 1.867)\tData  1.666 ( 1.666)\tLoss 5.3452e-02 (5.3452e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.392 ( 2.392)\tLoss 3.0871e-01 (3.0871e-01)\tAcc@1  87.11 ( 87.11)\n",
      " * Acc@1 87.205\n",
      "Epoch: [9][  0/375]\tTime  2.611 ( 2.611)\tData  2.364 ( 2.364)\tLoss 1.6009e-02 (1.6009e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.428 ( 2.428)\tLoss 5.2260e-01 (5.2260e-01)\tAcc@1  87.89 ( 87.89)\n",
      " * Acc@1 84.578\n",
      "Epoch: [10][  0/375]\tTime  2.597 ( 2.597)\tData  2.383 ( 2.383)\tLoss 4.5303e-02 (4.5303e-02)\tAcc@1  98.05 ( 98.05)\n",
      "Test: [  0/152]\tTime  2.520 ( 2.520)\tLoss 2.0361e-01 (2.0361e-01)\tAcc@1  91.80 ( 91.80)\n",
      " * Acc@1 92.611\n",
      "Epoch: [11][  0/375]\tTime  2.757 ( 2.757)\tData  2.505 ( 2.505)\tLoss 3.1772e-02 (3.1772e-02)\tAcc@1  98.83 ( 98.83)\n",
      "Test: [  0/152]\tTime  2.966 ( 2.966)\tLoss 4.2435e-01 (4.2435e-01)\tAcc@1  89.84 ( 89.84)\n",
      " * Acc@1 88.275\n",
      "Epoch: [12][  0/375]\tTime  3.115 ( 3.115)\tData  2.920 ( 2.920)\tLoss 2.8956e-02 (2.8956e-02)\tAcc@1  98.83 ( 98.83)\n",
      "Test: [  0/152]\tTime  1.819 ( 1.819)\tLoss 3.9229e-01 (3.9229e-01)\tAcc@1  89.45 ( 89.45)\n",
      " * Acc@1 90.765\n",
      "Epoch: [13][  0/375]\tTime  2.382 ( 2.382)\tData  2.155 ( 2.155)\tLoss 4.0316e-02 (4.0316e-02)\tAcc@1  98.83 ( 98.83)\n",
      "Test: [  0/152]\tTime  2.332 ( 2.332)\tLoss 2.9946e-01 (2.9946e-01)\tAcc@1  91.02 ( 91.02)\n",
      " * Acc@1 88.208\n",
      "Epoch: [14][  0/375]\tTime  2.610 ( 2.610)\tData  2.361 ( 2.361)\tLoss 4.5777e-02 (4.5777e-02)\tAcc@1  98.05 ( 98.05)\n",
      "Test: [  0/152]\tTime  2.080 ( 2.080)\tLoss 7.4845e-01 (7.4845e-01)\tAcc@1  83.98 ( 83.98)\n",
      " * Acc@1 84.477\n",
      "Epoch: [15][  0/375]\tTime  2.952 ( 2.952)\tData  2.750 ( 2.750)\tLoss 1.7865e-02 (1.7865e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.097 ( 2.097)\tLoss 2.4872e-01 (2.4872e-01)\tAcc@1  93.36 ( 93.36)\n",
      " * Acc@1 93.136\n",
      "Epoch: [16][  0/375]\tTime  2.171 ( 2.171)\tData  1.946 ( 1.946)\tLoss 2.6181e-02 (2.6181e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.635 ( 2.635)\tLoss 5.7617e-01 (5.7617e-01)\tAcc@1  90.23 ( 90.23)\n",
      " * Acc@1 91.016\n",
      "Epoch: [17][  0/375]\tTime  2.464 ( 2.464)\tData  2.223 ( 2.223)\tLoss 3.8812e-02 (3.8812e-02)\tAcc@1  98.05 ( 98.05)\n",
      "Test: [  0/152]\tTime  2.833 ( 2.833)\tLoss 1.9454e-01 (1.9454e-01)\tAcc@1  96.09 ( 96.09)\n",
      " * Acc@1 96.701\n",
      "Epoch: [18][  0/375]\tTime  2.457 ( 2.457)\tData  2.207 ( 2.207)\tLoss 1.4849e-02 (1.4849e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  3.035 ( 3.035)\tLoss 1.4147e-01 (1.4147e-01)\tAcc@1  97.27 ( 97.27)\n",
      " * Acc@1 95.659\n",
      "Epoch: [19][  0/375]\tTime  1.982 ( 1.982)\tData  1.783 ( 1.783)\tLoss 1.3578e-02 (1.3578e-02)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.597 ( 2.597)\tLoss 3.1179e-01 (3.1179e-01)\tAcc@1  93.75 ( 93.75)\n",
      " * Acc@1 93.637\n",
      "Epoch: [20][  0/375]\tTime  2.213 ( 2.213)\tData  1.995 ( 1.995)\tLoss 1.0597e-02 (1.0597e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.135 ( 2.135)\tLoss 1.4211e-01 (1.4211e-01)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 96.502\n",
      "Epoch: [21][  0/375]\tTime  3.087 ( 3.087)\tData  2.865 ( 2.865)\tLoss 6.3554e-03 (6.3554e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.579 ( 2.579)\tLoss 1.2084e-01 (1.2084e-01)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 96.474\n",
      "Epoch: [22][  0/375]\tTime  2.371 ( 2.371)\tData  2.154 ( 2.154)\tLoss 1.6120e-02 (1.6120e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.340 ( 2.340)\tLoss 1.9100e-01 (1.9100e-01)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 96.492\n",
      "Epoch: [23][  0/375]\tTime  2.188 ( 2.188)\tData  1.964 ( 1.964)\tLoss 1.0322e-02 (1.0322e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.550 ( 2.550)\tLoss 1.1404e-01 (1.1404e-01)\tAcc@1  97.27 ( 97.27)\n",
      " * Acc@1 96.515\n",
      "Epoch: [24][  0/375]\tTime  1.982 ( 1.982)\tData  1.767 ( 1.767)\tLoss 9.7053e-03 (9.7053e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.404 ( 2.404)\tLoss 2.0691e-01 (2.0691e-01)\tAcc@1  95.31 ( 95.31)\n",
      " * Acc@1 96.404\n",
      "Epoch: [25][  0/375]\tTime  1.796 ( 1.796)\tData  1.581 ( 1.581)\tLoss 3.0783e-02 (3.0783e-02)\tAcc@1  98.44 ( 98.44)\n",
      "Test: [  0/152]\tTime  2.358 ( 2.358)\tLoss 4.8819e-02 (4.8819e-02)\tAcc@1  98.44 ( 98.44)\n",
      " * Acc@1 96.528\n",
      "Epoch: [26][  0/375]\tTime  2.854 ( 2.854)\tData  2.634 ( 2.634)\tLoss 5.5869e-03 (5.5869e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  3.104 ( 3.104)\tLoss 1.2688e-01 (1.2688e-01)\tAcc@1  97.27 ( 97.27)\n",
      " * Acc@1 96.432\n",
      "Epoch: [27][  0/375]\tTime  2.503 ( 2.503)\tData  2.310 ( 2.310)\tLoss 7.2053e-03 (7.2053e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.105 ( 2.105)\tLoss 8.1169e-02 (8.1169e-02)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 96.561\n",
      "Epoch: [28][  0/375]\tTime  2.396 ( 2.396)\tData  2.183 ( 2.183)\tLoss 1.2374e-02 (1.2374e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.160 ( 2.160)\tLoss 1.3987e-01 (1.3987e-01)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 96.541\n",
      "Epoch: [29][  0/375]\tTime  2.226 ( 2.226)\tData  2.005 ( 2.005)\tLoss 1.2825e-02 (1.2825e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.868 ( 2.868)\tLoss 1.5950e-01 (1.5950e-01)\tAcc@1  96.09 ( 96.09)\n",
      " * Acc@1 96.530\n",
      "Test: [ 0/89]\tTime  2.248 ( 2.248)\tLoss 1.9275e-01 (1.9275e-01)\tAcc@1  96.09 ( 96.09)\n",
      " * Acc@1 97.208\n",
      "************* test number 1 ratio 0.8 acc97.20832061767578 *************\n",
      "Epoch: [0][  0/375]\tTime  2.512 ( 2.512)\tData  2.293 ( 2.293)\tLoss 1.0944e+00 (1.0944e+00)\tAcc@1  37.89 ( 37.89)\n",
      "Test: [  0/152]\tTime  2.417 ( 2.417)\tLoss 5.6052e-01 (5.6052e-01)\tAcc@1  80.86 ( 80.86)\n",
      " * Acc@1 79.850\n",
      "Epoch: [1][  0/375]\tTime  1.775 ( 1.775)\tData  1.566 ( 1.566)\tLoss 1.6578e-01 (1.6578e-01)\tAcc@1  93.75 ( 93.75)\n",
      "Test: [  0/152]\tTime  2.363 ( 2.363)\tLoss 6.2836e-01 (6.2836e-01)\tAcc@1  83.59 ( 83.59)\n",
      " * Acc@1 82.438\n",
      "Epoch: [2][  0/375]\tTime  2.408 ( 2.408)\tData  2.174 ( 2.174)\tLoss 8.1698e-02 (8.1698e-02)\tAcc@1  95.70 ( 95.70)\n",
      "Test: [  0/152]\tTime  2.097 ( 2.097)\tLoss 8.7590e-01 (8.7590e-01)\tAcc@1  82.42 ( 82.42)\n",
      " * Acc@1 82.849\n",
      "Epoch: [3][  0/375]\tTime  2.227 ( 2.227)\tData  2.028 ( 2.028)\tLoss 1.2333e-01 (1.2333e-01)\tAcc@1  95.31 ( 95.31)\n",
      "Test: [  0/152]\tTime  2.105 ( 2.105)\tLoss 3.1103e+00 (3.1103e+00)\tAcc@1  71.09 ( 71.09)\n",
      " * Acc@1 69.009\n",
      "Epoch: [4][  0/375]\tTime  2.593 ( 2.593)\tData  2.350 ( 2.350)\tLoss 7.1739e-02 (7.1739e-02)\tAcc@1  96.48 ( 96.48)\n",
      "Test: [  0/152]\tTime  2.317 ( 2.317)\tLoss 8.7961e-01 (8.7961e-01)\tAcc@1  82.81 ( 82.81)\n",
      " * Acc@1 80.372\n",
      "Epoch: [5][  0/375]\tTime  2.574 ( 2.574)\tData  2.366 ( 2.366)\tLoss 4.8611e-02 (4.8611e-02)\tAcc@1  98.44 ( 98.44)\n",
      "Test: [  0/152]\tTime  2.638 ( 2.638)\tLoss 4.6058e-01 (4.6058e-01)\tAcc@1  84.38 ( 84.38)\n",
      " * Acc@1 81.755\n",
      "Epoch: [6][  0/375]\tTime  2.490 ( 2.490)\tData  2.204 ( 2.204)\tLoss 6.8809e-02 (6.8809e-02)\tAcc@1  97.66 ( 97.66)\n",
      "Test: [  0/152]\tTime  2.218 ( 2.218)\tLoss 6.5710e+00 (6.5710e+00)\tAcc@1  63.28 ( 63.28)\n",
      " * Acc@1 62.326\n",
      "Epoch: [7][  0/375]\tTime  2.477 ( 2.477)\tData  2.255 ( 2.255)\tLoss 4.1387e-02 (4.1387e-02)\tAcc@1  98.44 ( 98.44)\n",
      "Test: [  0/152]\tTime  2.465 ( 2.465)\tLoss 6.0853e-01 (6.0853e-01)\tAcc@1  84.38 ( 84.38)\n",
      " * Acc@1 85.928\n",
      "Epoch: [8][  0/375]\tTime  2.093 ( 2.093)\tData  1.883 ( 1.883)\tLoss 4.3873e-02 (4.3873e-02)\tAcc@1  97.66 ( 97.66)\n",
      "Test: [  0/152]\tTime  2.662 ( 2.662)\tLoss 4.0943e+00 (4.0943e+00)\tAcc@1  62.50 ( 62.50)\n",
      " * Acc@1 62.197\n",
      "Epoch: [9][  0/375]\tTime  2.579 ( 2.579)\tData  2.273 ( 2.273)\tLoss 4.5596e-02 (4.5596e-02)\tAcc@1  98.83 ( 98.83)\n",
      "Test: [  0/152]\tTime  1.705 ( 1.705)\tLoss 8.2658e-01 (8.2658e-01)\tAcc@1  87.89 ( 87.89)\n",
      " * Acc@1 88.394\n",
      "Epoch: [10][  0/375]\tTime  2.757 ( 2.757)\tData  2.511 ( 2.511)\tLoss 4.1082e-02 (4.1082e-02)\tAcc@1  98.05 ( 98.05)\n",
      "Test: [  0/152]\tTime  2.377 ( 2.377)\tLoss 5.9200e-01 (5.9200e-01)\tAcc@1  88.67 ( 88.67)\n",
      " * Acc@1 89.948\n",
      "Epoch: [11][  0/375]\tTime  2.167 ( 2.167)\tData  1.936 ( 1.936)\tLoss 3.6978e-02 (3.6978e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.121 ( 2.121)\tLoss 3.3413e-01 (3.3413e-01)\tAcc@1  92.97 ( 92.97)\n",
      " * Acc@1 92.913\n",
      "Epoch: [12][  0/375]\tTime  2.068 ( 2.068)\tData  1.863 ( 1.863)\tLoss 4.2630e-02 (4.2630e-02)\tAcc@1  98.05 ( 98.05)\n",
      "Test: [  0/152]\tTime  2.061 ( 2.061)\tLoss 1.7902e+00 (1.7902e+00)\tAcc@1  87.50 ( 87.50)\n",
      " * Acc@1 83.761\n",
      "Epoch: [13][  0/375]\tTime  1.800 ( 1.800)\tData  1.591 ( 1.591)\tLoss 1.3718e-02 (1.3718e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.247 ( 2.247)\tLoss 2.0276e-01 (2.0276e-01)\tAcc@1  94.14 ( 94.14)\n",
      " * Acc@1 93.270\n",
      "Epoch: [14][  0/375]\tTime  1.740 ( 1.740)\tData  1.531 ( 1.531)\tLoss 1.3511e-02 (1.3511e-02)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.591 ( 2.591)\tLoss 3.1713e-01 (3.1713e-01)\tAcc@1  93.36 ( 93.36)\n",
      " * Acc@1 92.988\n",
      "Epoch: [15][  0/375]\tTime  3.074 ( 3.074)\tData  2.874 ( 2.874)\tLoss 2.1323e-02 (2.1323e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  1.881 ( 1.881)\tLoss 2.6734e-01 (2.6734e-01)\tAcc@1  93.75 ( 93.75)\n",
      " * Acc@1 93.699\n",
      "Epoch: [16][  0/375]\tTime  1.849 ( 1.849)\tData  1.656 ( 1.656)\tLoss 2.2907e-02 (2.2907e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.109 ( 2.109)\tLoss 4.4525e-01 (4.4525e-01)\tAcc@1  91.02 ( 91.02)\n",
      " * Acc@1 91.988\n",
      "Epoch: [17][  0/375]\tTime  2.374 ( 2.374)\tData  2.149 ( 2.149)\tLoss 4.2596e-02 (4.2596e-02)\tAcc@1  98.83 ( 98.83)\n",
      "Test: [  0/152]\tTime  2.351 ( 2.351)\tLoss 2.4784e-01 (2.4784e-01)\tAcc@1  96.09 ( 96.09)\n",
      " * Acc@1 95.657\n",
      "Epoch: [18][  0/375]\tTime  2.145 ( 2.145)\tData  1.948 ( 1.948)\tLoss 1.0836e-02 (1.0836e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.334 ( 2.334)\tLoss 1.9886e-01 (1.9886e-01)\tAcc@1  94.53 ( 94.53)\n",
      " * Acc@1 96.629\n",
      "Epoch: [19][  0/375]\tTime  2.295 ( 2.295)\tData  2.071 ( 2.071)\tLoss 7.6197e-03 (7.6197e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.269 ( 2.269)\tLoss 7.6976e-02 (7.6976e-02)\tAcc@1  96.09 ( 96.09)\n",
      " * Acc@1 96.722\n",
      "Epoch: [20][  0/375]\tTime  3.046 ( 3.046)\tData  2.784 ( 2.784)\tLoss 1.8836e-02 (1.8836e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.547 ( 2.547)\tLoss 7.3414e-02 (7.3414e-02)\tAcc@1  98.44 ( 98.44)\n",
      " * Acc@1 96.779\n",
      "Epoch: [21][  0/375]\tTime  2.202 ( 2.202)\tData  2.008 ( 2.008)\tLoss 1.6369e-02 (1.6369e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.335 ( 2.335)\tLoss 1.4378e-01 (1.4378e-01)\tAcc@1  95.70 ( 95.70)\n",
      " * Acc@1 96.742\n",
      "Epoch: [22][  0/375]\tTime  2.555 ( 2.555)\tData  2.310 ( 2.310)\tLoss 1.2597e-02 (1.2597e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  1.933 ( 1.933)\tLoss 1.0628e-01 (1.0628e-01)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 96.435\n",
      "Epoch: [23][  0/375]\tTime  2.529 ( 2.529)\tData  2.321 ( 2.321)\tLoss 1.1013e-02 (1.1013e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.846 ( 2.846)\tLoss 1.0525e-01 (1.0525e-01)\tAcc@1  97.27 ( 97.27)\n",
      " * Acc@1 96.344\n",
      "Epoch: [24][  0/375]\tTime  2.322 ( 2.322)\tData  2.095 ( 2.095)\tLoss 2.4584e-02 (2.4584e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  2.602 ( 2.602)\tLoss 7.5999e-02 (7.5999e-02)\tAcc@1  98.05 ( 98.05)\n",
      " * Acc@1 96.750\n",
      "Epoch: [25][  0/375]\tTime  2.191 ( 2.191)\tData  2.028 ( 2.028)\tLoss 2.5473e-02 (2.5473e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  1.955 ( 1.955)\tLoss 1.6622e-01 (1.6622e-01)\tAcc@1  95.70 ( 95.70)\n",
      " * Acc@1 96.735\n",
      "Epoch: [26][  0/375]\tTime  1.916 ( 1.916)\tData  1.711 ( 1.711)\tLoss 4.4275e-03 (4.4275e-03)\tAcc@1 100.00 (100.00)\n",
      "Test: [  0/152]\tTime  2.481 ( 2.481)\tLoss 1.0758e-01 (1.0758e-01)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 96.779\n",
      "Epoch: [27][  0/375]\tTime  2.616 ( 2.616)\tData  2.377 ( 2.377)\tLoss 1.9383e-02 (1.9383e-02)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.387 ( 2.387)\tLoss 7.9794e-02 (7.9794e-02)\tAcc@1  97.66 ( 97.66)\n",
      " * Acc@1 96.546\n",
      "Epoch: [28][  0/375]\tTime  2.207 ( 2.207)\tData  1.996 ( 1.996)\tLoss 1.9645e-02 (1.9645e-02)\tAcc@1  99.22 ( 99.22)\n",
      "Test: [  0/152]\tTime  1.877 ( 1.877)\tLoss 1.5896e-01 (1.5896e-01)\tAcc@1  96.09 ( 96.09)\n",
      " * Acc@1 96.830\n",
      "Epoch: [29][  0/375]\tTime  2.696 ( 2.696)\tData  2.494 ( 2.494)\tLoss 8.3135e-03 (8.3135e-03)\tAcc@1  99.61 ( 99.61)\n",
      "Test: [  0/152]\tTime  2.559 ( 2.559)\tLoss 1.0209e-01 (1.0209e-01)\tAcc@1  96.48 ( 96.48)\n",
      " * Acc@1 96.665\n",
      "Test: [ 0/89]\tTime  2.858 ( 2.858)\tLoss 1.0258e-01 (1.0258e-01)\tAcc@1  96.09 ( 96.09)\n",
      " * Acc@1 97.147\n",
      "************* test number 2 ratio 0.8 acc97.1468734741211 *************\n"
     ]
    }
   ],
   "source": [
    "ratio = [.8, .8, .8]\n",
    "acc_list = []\n",
    "test_acc = []\n",
    "\n",
    "for num in range(len(ratio)) :\n",
    "    args.saved_dir = '../trained_models/resnet8/colon1_' + str(num) + '/checkpoint.pt'\n",
    "    path = Path(args.saved_dir.split('checkpoint')[0])\n",
    "    path.mkdir(parents=True, exist_ok=True)  \n",
    "    \n",
    "    model = resnet18(pretrained=False)\n",
    "    # model.fc.out_features = 3\n",
    "    model.fc = nn.Linear(64, 3)\n",
    "    torch.nn.init.normal_(model.fc.weight, std=0.01)\n",
    "    torch.nn.init.constant_(model.fc.bias, 0.)    \n",
    "    # model.load_state_dict(default_state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "#     optimizer = torch.optim.Adam(params, lr=0.01, weight_decay=args.weight_decay)\n",
    "    optimizer = torch.optim.SGD(\n",
    "           params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15, 20, 25], \n",
    "                                                        gamma=0.2)\n",
    "    # criterion = nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    best_acc1 = 0\n",
    "    acc1 = 0\n",
    "    train_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "#         print('lr', optimizer.param_groups[0]['lr'])\n",
    "        losses = train(train_loader, model, criterion, optimizer, epoch, args)\n",
    "        scheduler.step()\n",
    "\n",
    "        # evaluate after every epoch\n",
    "        acc1 = validate(val_loader, model, criterion, args)   \n",
    "        val_acc.append(acc1.item())\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > best_acc1\n",
    "        best_acc1 = max(acc1, best_acc1) \n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "            'scheduler' : scheduler.state_dict(),\n",
    "        }, is_best, filename=args.saved_dir)   \n",
    "        \n",
    "    checkpoint = torch.load(args.saved_dir)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    acc1 = validate(test_loader, model, criterion, args)    \n",
    "    print('************* test number {} ratio {} acc{} *************'.format(num, ratio[num], acc1))    \n",
    "    acc_list.append(val_acc)    \n",
    "    test_acc.append(acc1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc41c0-29ea-4aea-aeea-07a3fc813ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(args.epochs)\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.rcParams['lines.linewidth'] = 1\n",
    "plt.plot(epochs, acc_list[0], linestyle='--')\n",
    "plt.plot(epochs, acc_list[1])\n",
    "plt.plot(epochs, acc_list[2])\n",
    "# plt.plot(epochs, acc_list[3])\n",
    "plt.legend(['.4', '.4', '.4' ])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('val acc')\n",
    "plt.title('val acc with stain augment using resnet18 for stomach')\n",
    "# plt.axis([10, 30, 88, 94])\n",
    "plt.show()\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451bd47-e60e-4ae6-8eed-f8111f3c89e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
